TWO-STAGE SWEEP RESULTS (2026-02-16)
================================================================================

STAGE 0: Feature Selection
- Reduced from 58 to 20 features (top by importance)
- Top features: volume_surge_4h, tick_direction_ratio_24h, rsi_divergence_14h_168h

STAGE 1: Screening (54 configs, single 80/20 split)
- CatBoost dominates: 8 of top-10 configs
- Best screening Sharpe: 0.336 (CatBoost d=3 lr=0.01 l2=1.0)
- LightGBM: uniformly negative Sharpe at d>3
- XGBoost: mostly negative, best 0.256

STAGE 2: Walk-Forward Validation (top 5)
- Best: CatBoost d=4 lr=0.01 l2=1.0
  → Mean Sharpe 0.98 ± 0.73
  → Accuracy 53.0%
  → By year: 2019=1.55, 2020=2.62, 2021=0.53, 2022=-2.02, 2023=1.31
  
- Runner-up: CatBoost d=4 lr=0.01 l2=3.0
  → Mean Sharpe 0.95 ± 1.10
  → Accuracy 52.8%

COMPARISON TO BASELINE
- Multi-TF Donchian: Sharpe 1.06
- Best ML (CatBoost): Sharpe 0.98
- ML does NOT beat baseline (6% worse)

KEY FINDINGS
1. CatBoost >> LightGBM > XGBoost for this task
2. Shallow trees (d=3-4) outperform deep (d=5)
3. Low learning rates (0.01) best
4. High variance across years (std 0.73-1.10)
5. 2022 bear market: all configs negative Sharpe
6. Accuracy 53-54% = marginal edge

TIER ASSESSMENT: TIER 3
- Sharpe 0.98 ≥ 0.4 threshold
- Shows edge over B&H
- Does not beat baseline strategy
- High variance suggests overfitting risk
- Action: Continue iterating (try more features, ensemble methods, different train windows)

